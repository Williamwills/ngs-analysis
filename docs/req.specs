NGS Analysis Pipeline Framework Specifications

- Transparency - Able to see and edit what goes on in the data processing pipeline
- Easily modifiable for user's needs
- Run modes
  - Single node - define max number of threads/processes to use
  - Cluster - SGE
  - Define max memory
  - Simulation runs?
  - Output run status
- Option for samtools (bamtools?) instead of picard
- Record benchmarking data for each step
  - reads, sequences
  - time
  - file sizes
  - memory use
- Generate QC and results reports

======================================================================================
Setup/Install

Edit ~/.bash_profile to contain the following lines

# NGS Analysis Pipeline
export NGS_ANALYSIS_CONFIG=path/to/ngs-analysis/ngs.config.sh
PATH=path/to/ngs-analysis/pipelines:$PATH
PATH=path/to/ngs-analysis/modules/util:$PATH

======================================================================================


Workflow
SETUP-----------------------
1. Clone local git repo
2. Edit .bash_profile (see above)
3. Edit ngs.config.sh
----------------------------
4. Run pipeline scripts
5. Custom analyses parameters
	a. copy ngs.config.sh to another place
	b. edit parameters in ngs.config.sh
	c. Source ngs.config.sh
	d. set $NGS_ANALYSIS_CONFIG to the new ngs.config.sh
	e. Re-run individual scripts/pipelines


copy $NGS_ANALYSIS_CONFIG

======================================================================================

Tools list

GATK
Cutadapt
Picard Tools
Samtools
Varscan
Somatic Sniper
Pindel
Dindel

======================================================================================

Directory Structure

- Ngs-analysis
  - docs:  documentation
  - lib: library of functions and classes for python, bash, etc
    - bash
    - python
  - modules: contain scripts used by the pipeline framework, also for storing user-written scripts for each ngs project
    - util: Utility tools for various non-specific tasks
    - seq
    - align
    - variant
    - annot
    - somatic
  - configure.sh: script for generating the workspace's necessary features, such as the directory structure, setting the right paths, etc
  - *data: contain all the files generated during the output of the pipeline
  - *tmp: contain intermediate files that are not important, but necessary for the pipeline
  - *reports: contain QC and other human-readable reports for the project

* indicates directories generated by running configure.sh

======================================================================================

Python Library

- seq.py
- filesys.py

======================================================================================

Required Python Modules

- argparse
- gzip
- os
- sys
- unittest
- zipfile

======================================================================================

Development Steps

- SGE Pipeline
  - Pipeline to go from basecalling to vcf calls
    - Script to generate symbolic links to all the sample fastq sequences, and to rename them to format sample.R1.fastq.gz
    - Use samplesheet to generate adapter sequence input for cutadapt.sh
    - Sanity check for SampleSheet.csv
      - same index for same lane
      - length of index sequence
      - Remove space, commas, forward or backward slashes
  - Unit tests for python library
  - Testing pipeline
    - cutadapt
    - sickle
      - make sure sickle outputs gzipped fastq
  - Fastq scores summary
    - positional averages
    - cumulative distribution of scores
  - Documentation of each step in detail
  - Documentation of python library
    - Class objects, functions
    - XML format standards
- QC Reporting tools
  - Generate sequence statistics, alignment reports as HTML
    - R graphics library
    - jinja2
- Statistics gathering and reports
  - runtime, memory, run status
- Improvements to SGE Pipeline
  - Benchmark various read counts
  - Resource usage requests based on benchmarking data
  - Runtime optimization
    - Merge fastq_scores.py with fastq_stats.py to save time on file io.
  - Pipeline visualization (dot)
  - Scripts for checking to make sure that all the tools exists in the paths defined
- Single node run mode


NEED TO TEST
seq/
	casava.bcl2fastq.ga.sh
	cutadapt.sh
	fastq_scores.py
	fastq_stats.py
	
align/
	picard.fixmate.sh
	picard.sam2bam.sh
	picard.sortsam.sh



======================================================================================

Tutorials

Git

Clone
git clone https://jjinking@code.google.com/p/ngs-analysis/

Check remote repos
git remote -v

Update from remote repo
git pull

Clone from a local clone
git clone ngs-analysis blah

Check status
git status

Add new file/directory
git add docs

Remove file
git rm foo

Untrack files
git rm --cached filename

Commit to local repo
git commit -a -m 'Created docs and removed file foo'

Upload to main git repo server
git push origin master

See which branch I'm on, and list all branches
git branch

Ignore current changes, but pull from origin repo
git stash
git pull
git stash apply

----------------------------------------------------------------

Python Unit Test

- In ngs-analysis base directory, run the configuration script to set all the paths, etc
- Go to ngs-analysis/lib/python/ngs.test directory
- Run each script as a command-line tool
- Batch?

----------------------------------------------------------------

Creating Shell Scripts

Example shell script: modules/util/example.sh



-----------------------------------------------------------
Improvements over minmin

basecalling third option for number of threads
automatic generation of reports/images/plots